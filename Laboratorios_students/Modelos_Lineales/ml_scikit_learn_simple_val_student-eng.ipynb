{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reminders on scikit-learn and machine learning (correction)\n",
    "\n",
    "Inspired by Xavier Dupr√©'s course\n",
    "\n",
    "A few simple exercises on *scikit-learn*. The notebook is long for those who are new to machine learning and probably without suspense for those who have already done some."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Synthetic dataset\n",
    "\n",
    "We simulate a set of random data, with a uniform distribution $\\mathcal{U}_{(0,1)}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.02037951, 0.30968762],\n",
       "       [0.29413   , 0.37153774],\n",
       "       [0.41002219, 0.33296235],\n",
       "       [0.86395508, 0.87805471],\n",
       "       [0.94593263, 0.3296339 ]])"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from numpy import random\n",
    "n = 1000\n",
    "X = random.rand(n, 2)\n",
    "X[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's create a starting model: $Y = 3 X_1 - 2 X_2^2 + \\epsilon$.\n",
    "\n",
    "We'll need to approximate $Y$ using the descriptors $X_1$ and $X_2$. \n",
    "\n",
    "$\\epsilon $~$ \\mathcal{U}_{(0,1)}$ is a source of noise that we can't control for prediction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-2.99442799e-03,  1.27015060e+00,  1.85177846e+00,  1.78228679e+00,\n",
       "        3.38708163e+00])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y = X[:, 0] * 3 - 2 * X[:, 1] ** 2 + random.rand(n)\n",
    "y[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 1: dividing into training and test databases\n",
    "\n",
    "We need to test our model on a different database from the one used for training **in order to measure its power of generalization**. As we have seen, the empirical risk on a given set of data is not characteristic of the general risk, and we may witness a phenomenon of over-learning on the training set.\n",
    "\n",
    "In our case, we want the model to learn the law $3 X_1 - 2 X_2^2$ and **overlearning would be equivalent to memorizing the noise vector $\\epsilon$** which only corresponds to variations in $Y$ that are independent of our model.\n",
    "\n",
    "Simple [train_test_split](http://scikit-learn.org/stable/modules/generated/sklearn.model_selection.train_test_split.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# to fill"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercice 2: learn a linear regression\n",
    "\n",
    "Find the weights $\\theta = \\begin{pmatrix}\n",
    "           \\theta_{1} \\\\\n",
    "           \\theta_{2}\n",
    "         \\end{pmatrix}$ solution de $\\underset{\\theta}{\\arg\\max} \\sum_{i=1}^{n}|Y_i-f_{\\theta}(\\mathbf{X}_i)|^2$\n",
    "         \n",
    "Where $f_{\\theta}(\\mathbf{X}) = \\theta_0 + \\sum_{d=1}^{D}\\theta_d X_d$ with in our case $D=2$\n",
    "\n",
    "Calculate the coefficient $R^2$. \n",
    "$$R^2=1-\\frac{\\sum_{i=1}^{n}|Y_i-f(\\mathbf{X}_i)|^2}{\\sum_{i=1}^{n}|Y_i-\\overline{Y}|^2}$$\n",
    "\n",
    "Where $\\mathbf{X} = \\begin{pmatrix}\n",
    "           X_{1} \\\\\n",
    "           X_{2}\n",
    "         \\end{pmatrix}$ et $\\overline{Y}=\\frac{1}{n}\\sum_{i=1}^{n}Y_i$\n",
    "\n",
    "Use : [LinearRegression](http://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LinearRegression.html), [r2_score](http://scikit-learn.org/stable/modules/generated/sklearn.metrics.r2_score.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# to fill"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 3: improve the model by applying a well-chosen transformation\n",
    "\n",
    "The starting model is: $Y = 3 X_1 - 2 X_2^2 + \\epsilon$. Simply add polynomial features with [PolynomialFeatures](http://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.PolynomialFeatures.html).\n",
    "\n",
    "Taking the parameter :\n",
    "```python\n",
    "degree=2\n",
    "```\n",
    "The intial descriptor set being $\\mathbf{X} = \\begin{pmatrix} X_{1} \\\\ X_{2} \\end{pmatrix}$ \n",
    "will now become $\\mathbf{X} = \\begin{pmatrix} 1 \\\\ X_{1} \\\\ X_{2} \\\\ X_{1}^2 \\\\ X_{1}X_{2} \\\\ X_{2}^2 \\end{pmatrix}$ which gives : \n",
    "\n",
    "$$f_{\\theta}(\\mathbf{X}) = \\theta'_0 + \\theta_1 X_1 + \\theta_2 X_2 + \\theta_3 X_1^2 + \\theta_4 X_1X_2 + \\theta_5 X_2^2$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# to fill"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 4: learn a random forest\n",
    "\n",
    "Use: [RandomForestRegressor](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestRegressor.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda2/lib/python2.7/site-packages/sklearn/ensemble/weight_boosting.py:29: DeprecationWarning: numpy.core.umath_tests is an internal NumPy module and should not be imported. It will be removed in a future NumPy release.\n",
      "  from numpy.core.umath_tests import inner1d\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "RandomForestRegressor(bootstrap=True, criterion='mse', max_depth=None,\n",
       "           max_features='auto', max_leaf_nodes=None,\n",
       "           min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "           min_samples_leaf=1, min_samples_split=2,\n",
       "           min_weight_fraction_leaf=0.0, n_estimators=10, n_jobs=1,\n",
       "           oob_score=False, random_state=None, verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor\n",
    "clf = RandomForestRegressor() \n",
    "# learning classifier \n",
    "clf.fit(X_train, y_train)\n",
    "# scoring classifier \n",
    "clf.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 5: A bit of math\n",
    "\n",
    "Compare the two models on the following data? What do you notice? Why or why not?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test2 = random.rand(n, 2) + 0.5\n",
    "y_test2 = X_test2[:, 0] * 3 - 2 * X_test2[:, 1] ** 2 + random.rand(n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# to fill"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 6: making a graph with...\n",
    "\n",
    "The scatterplot of the first and second game, the predictions of the two models, a legend, a title... with [pandas](https://pandas.pydata.org/) or directly with [matplotlib](https://matplotlib.org/) as you wish."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# to fill"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 7: Illustrating overfitting with a decision tree\n",
    "\n",
    "As the complexity of the model increases, overfitting again occurs. Similarly, the model using only $X_1$ and $X_2$ is not necessarily adapted to the problem and is in a case of underlearning. \n",
    "\n",
    "<img src=\"images/ex_over-underfitting.png\"> \n",
    "\n",
    "On the first data set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeRegressor\n",
    "\n",
    "res = []\n",
    "for md in range(1, 20):\n",
    "    # to fill\n",
    "    \n",
    "    res.append(dict(profondeur=md, r2_train=r2_train, r2_test=r2_test))\n",
    "\n",
    "df = pandas.DataFrame(res)\n",
    "df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ax = df.plot(x='profondeur', y=['r2_train', 'r2_test'])\n",
    "ax.set_title(\"Evolution du R2 selon la profondeur\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 8: Increasing the number of features and regularizing a logistic regression\n",
    "\n",
    "The aim is to examine the impact of regularizing the coefficients of a logistic regression as the number of features increases. We use polynomial features and a logistic regression. [Ridge](http://scikit-learn.org/stable/modules/generated/sklearn.linear_model.Ridge.html) or [Lasso](http://scikit-learn.org/stable/modules/generated/sklearn.linear_model.Lasso.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import Ridge, Lasso\n",
    "import numpy.linalg as nplin\n",
    "import numpy\n",
    "\n",
    "def coef_non_zero(coef):\n",
    "    return sum(numpy.abs(coef) > 0.001)\n",
    "\n",
    "res = []\n",
    "for d in range(1, 21):\n",
    "    # to fill\n",
    "\n",
    "df = pandas.DataFrame(res)\n",
    "df.head(21)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Make 2 graphs with :\n",
    "- Graph 1: number of features \n",
    "- Graph 2: number of non-zero coefficients\n",
    "As a function of the number of features in the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1, 2, figsize=(12, 4))\n",
    "# to fill"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
